{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pyrealsense2 as rs\n",
    "import open3d as o3d\n",
    "o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Debug)\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "# Used to display Matplotlib plots in Jupyter\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# PIL used to save images as pngs\n",
    "from PIL import Image\n",
    "\n",
    "# Robotics Toolbox used to determine camera coordinate frame given joint angles\n",
    "import roboticstoolbox as rtb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport rmlib\\nfrom rmlib.rmtools import utils, poses\\n\\nclass UR5_Interface():\\n    # Interface with the UR5 using RMLIB\\n    def __init__(self):\\n        print(\"Ensure RTDE3 is started on the SmartHand\")\\n        self.robot = rmlib.robot.Robot()\\n        self.arm = self.robot.arm \\n        self.gripper = self.robot.hand\\n        \\n    def moveHome(self):\\n        # Moves the arm linearly in cartesian space to home pose\\n        homePose = np.array([[ 0.99955322, -0.02418213, -0.01756664,  0.01498893],\\n                             [-0.01748495,0.00358545,-0.9998407,-0.57686779],\\n                             [0.02424126,0.99970114,0.00316103,0.05545535],\\n                             [0,0,0,1]])\\n        self.arm.move(target=homePose,move_type=\"l\")\\n    \\n    def openGripper(self):\\n        self.gripper.release()\\n    \\n    def closeGripper(self):\\n        self.gripper.close\\n        \\n    def routine(self):\\n        self.moveHome()\\n        initPose = self.arm.get_tcp_pose()\\n        print(f\"initPose: \\n {initPose}\")\\n        # translate forward to block\\n        dX,dY,dZ = 0,0,0.05\\n        goalPose = poses.translate_pose(initPose,translation_vec=[dX,dY,dZ],dir_pose=\"origin\")\\n        print(f\"goalPose:\\n{goalPose}\")\\n        self.arm.move(target=goalPose,move_type=\"l\")\\n        print(f\"finalPose: \\n{arm.get_tcp_pose()}\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMlib is used to interface with the Smarthand and the ur5\n",
    "'''\n",
    "import rmlib\n",
    "from rmlib.rmtools import utils, poses\n",
    "\n",
    "class UR5_Interface():\n",
    "    # Interface with the UR5 using RMLIB\n",
    "    def __init__(self):\n",
    "        print(\"Ensure RTDE3 is started on the SmartHand\")\n",
    "        self.robot = rmlib.robot.Robot()\n",
    "        self.arm = self.robot.arm \n",
    "        self.gripper = self.robot.hand\n",
    "        \n",
    "    def moveHome(self):\n",
    "        # Moves the arm linearly in cartesian space to home pose\n",
    "        homePose = np.array([[ 0.99955322, -0.02418213, -0.01756664,  0.01498893],\n",
    "                             [-0.01748495,0.00358545,-0.9998407,-0.57686779],\n",
    "                             [0.02424126,0.99970114,0.00316103,0.05545535],\n",
    "                             [0,0,0,1]])\n",
    "        self.arm.move(target=homePose,move_type=\"l\")\n",
    "    \n",
    "    def openGripper(self):\n",
    "        self.gripper.release()\n",
    "    \n",
    "    def closeGripper(self):\n",
    "        self.gripper.close\n",
    "        \n",
    "    def routine(self):\n",
    "        self.moveHome()\n",
    "        initPose = self.arm.get_tcp_pose()\n",
    "        print(f\"initPose: \\n {initPose}\")\n",
    "        # translate forward to block\n",
    "        dX,dY,dZ = 0,0,0.05\n",
    "        goalPose = poses.translate_pose(initPose,translation_vec=[dX,dY,dZ],dir_pose=\"origin\")\n",
    "        print(f\"goalPose:\\n{goalPose}\")\n",
    "        self.arm.move(target=goalPose,move_type=\"l\")\n",
    "        print(f\"finalPose: \\n{arm.get_tcp_pose()}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RTB_Model():\n",
    "    # Kinematic Model of the Robot in the Robotics Toolbox for Python (RTB)\n",
    "    def __init__(self):\n",
    "        # Model has units in meters\n",
    "        self.ur5 = rtb.models.DH.UR5()\n",
    "        homePose = np.array(np.radians([53,-112,144,-27.5,55,171.7]))\n",
    "        self.ur5.q = homePose\n",
    "    \n",
    "    def setJointAngles(self,thetas):\n",
    "        # thetas: 6 x 1 numpy array of joint angles (radians)\n",
    "        self.ur5.q = thetas\n",
    "        \n",
    "    \n",
    "    def plotRobot(self):\n",
    "        # Displays robot in matplotlib\n",
    "        ur5 = self.ur5\n",
    "        env = ur5.plot(ur5.q) # PyPlot backend\n",
    "        T_C = self.getCameraFrame()\n",
    "        T_C.plot(frame=\"C\",length=0.1)\n",
    "        env.hold()\n",
    "        \n",
    "    \n",
    "    def getCameraFrame(self):\n",
    "        # Robot joint angles need to be set pior \n",
    "        # Returns a SE3 SPatial Math Object (4 x 4 Homogenous Transform) corresponding to the robot's camera frame \n",
    "        T_N = self.ur5.fkine_all(self.ur5.q)[-1] # T_N - end-effector frame (before optoforce/gripper)\n",
    "        d = 0.1 # distance between end-effector frame origin and center of camera frame along z-axis (m)\n",
    "        P_C = np.array([0,0,d]) # Translation from frame T_N to origin of camera frame (m)\n",
    "        # theta = np.radians(90) # Rotation about the Z-axis between the camera frame and end-effector frame\n",
    "        T_C = T_N # * SE3.Tz(d) * SE3.Rz(theta) # Camera coordinate frame\n",
    "        return T_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  \u001b[38;5;1m-0.9947  \u001b[0m \u001b[38;5;1m-0.09739 \u001b[0m \u001b[38;5;1m-0.03338 \u001b[0m \u001b[38;5;4m-0.01549 \u001b[0m  \u001b[0m\n",
       "  \u001b[38;5;1m 0.02689 \u001b[0m \u001b[38;5;1m 0.06725 \u001b[0m \u001b[38;5;1m-0.9974  \u001b[0m \u001b[38;5;4m-0.2804  \u001b[0m  \u001b[0m\n",
       "  \u001b[38;5;1m 0.09938 \u001b[0m \u001b[38;5;1m-0.993   \u001b[0m \u001b[38;5;1m-0.06427 \u001b[0m \u001b[38;5;4m 0.176   \u001b[0m  \u001b[0m\n",
       "  \u001b[38;5;244m 0       \u001b[0m \u001b[38;5;244m 0       \u001b[0m \u001b[38;5;244m 0       \u001b[0m \u001b[38;5;244m 1       \u001b[0m  \u001b[0m\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = RTB_Model()\n",
    "r.getCameraFrame()\n",
    "# r.plotRobot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RealSense():\n",
    "    def __init__(self,zMax=1):\n",
    "        self.pinholeIntrinsics = None\n",
    "        self.zMax = zMax # max distance for objects in depth images (m)\n",
    "        # downsample point cloud with voxel size = 1 mm (0.001 m / 0.04 in)\n",
    "        self.voxelSize = 0.001\n",
    "        self.pcd = o3d.geometry.PointCloud() # current pcd from realsense\n",
    "        # self.extrinsics = None # extrinsic parameters of the camera frame 4 x 4 numpy matrix\n",
    "        self.extrinsics = np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]])\n",
    "        self.pipe,self.config = None,None\n",
    "    \n",
    "    def initConnection(self):\n",
    "        # Initializes connection to realsense, sets pipe,config values\n",
    "        self.pipe = rs.pipeline()\n",
    "        self.config = rs.config()\n",
    "\n",
    "        # Getting information about the connected realsense model (device object) - D405\n",
    "        pipeProfile = self.config.resolve(rs.pipeline_wrapper(self.pipe))\n",
    "        device = pipeProfile.get_device()\n",
    "        depth_sensor = device.first_depth_sensor()\n",
    "        self.depthScale = depth_sensor.get_depth_scale()\n",
    "        # print(depth_scale)\n",
    "\n",
    "        # 1 - default, 2 - hand, 3 - high accuracy, 4 - high density, 5 - medium density\n",
    "        depth_sensor.set_option(rs.option.visual_preset,4) # 4 corresponds to high-density option\n",
    "\n",
    "        # Setting attributes for stream\n",
    "        # Depth Stream (1280 x 720) 5 fps - D405 Sensor has max 1280 x 720\n",
    "        # (Minimum z depth is between 55-70 mm)\n",
    "        self.config.enable_stream(rs.stream.depth,1280,720,rs.format.z16,5)\n",
    "\n",
    "        # Color and Infrared D405 Streams Available (1280 x 720) 5 fps - D405 Sensor has max 1280 x 720\n",
    "        self.config.enable_stream(rs.stream.color,1280,720,rs.format.rgb8,5)\n",
    "\n",
    "        # Starting the pipeline based on the specified configuration\n",
    "        self.pipe.start(self.config)\n",
    "        \n",
    "    def getPinholeInstrinsics(self,frame):\n",
    "        # frame is a subclass of pyrealsense2.video_frame (depth_frame,etc)\n",
    "        intrinsics = frame.profile.as_video_stream_profile().intrinsics\n",
    "        return o3d.camera.PinholeCameraIntrinsic(intrinsics.width,intrinsics.height, intrinsics.fx,\n",
    "                                                intrinsics.fy, intrinsics.ppx,\n",
    "                                                intrinsics.ppy)\n",
    "\n",
    "\n",
    "    def takeImages(self,saveImages=False):\n",
    "        # returns Open3D RGBD Image\n",
    "        # Starting the pipeline based on the specified configuration\n",
    "        pipe,config = self.pipe,self.config\n",
    "        frames = pipe.wait_for_frames()\n",
    "        depthFrame = frames.get_depth_frame() # pyrealsense2.depth_frame\n",
    "        colorFrame = frames.get_color_frame()\n",
    "        \n",
    "        self.pinholeInstrinsics = self.getPinholeInstrinsics(colorFrame)\n",
    "        # asign extrinsics here if the camera pose is known\n",
    "        # alignOperator maps depth frames to color frames\n",
    "        alignOperator = rs.align(rs.stream.color)\n",
    "        alignOperator.process(frames)\n",
    "        alignedDepthFrame,alignedColorFrame = frames.get_depth_frame(),frames.get_color_frame()\n",
    "        \n",
    "        # unmodified rgb and z images as numpy arrays of 3 and 1 channels\n",
    "        rawColorImage = np.array(alignedColorFrame.get_data())\n",
    "        rawDepthImage = np.asarray(alignedDepthFrame.get_data())\n",
    "\n",
    "        rawRGBDImage = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "            o3d.geometry.Image(rawColorImage),\n",
    "            o3d.geometry.Image(rawDepthImage.astype('uint16')),\n",
    "            depth_scale=1.0 / self.depthScale,\n",
    "            depth_trunc = self.zMax,\n",
    "            convert_rgb_to_intensity=False)\n",
    "        \n",
    "        subFix = \"Back\" # append to end of file paths\n",
    "        if saveImages:\n",
    "            np.save(f\"newRealsense/depthImage{subFix}\",rawRGBDImage.depth)\n",
    "            np.save(f\"newRealsense/colorImage{subFix}\",rawRGBDImage.color)\n",
    "            colorIM = Image.fromarray(rawColorImage)\n",
    "            colorIM.save(f\"newRealsense/colorImage{subFix}.jpeg\")\n",
    "        return rawRGBDImage\n",
    "\n",
    "    def getPCD(self,saveData=False):\n",
    "        # depthImage: 1 channel numpy array of z values\n",
    "        # colorImage: 3 channel numpy array of rgb values\n",
    "        # zMax: disance at which points are cut off\n",
    "        # display: boolean, toggles if point cloud should be shown\n",
    "        # out: tuple of (open3d point cloud (o3d.geometry.PointCloud),RGBDImage)\n",
    "        rawRGBDImage = self.takeImages()\n",
    "        if self.extrinsics is None == False:\n",
    "            pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rawRGBDImage,self.pinholeInstrinsics,self.extrinsics,project_valid_depth_only=True) # extrinsics pose\n",
    "        else:\n",
    "            pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rawRGBDImage,self.pinholeInstrinsics,project_valid_depth_only=True) # default pose\n",
    "        # flip_transform = [[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]]\n",
    "        # pcd.transform(flip_transform)\n",
    "        downsampledPCD = pcd.voxel_down_sample(voxel_size=self.voxelSize)\n",
    "        if saveData:\n",
    "            subFix = \"2\"\n",
    "            np.save(f\"colorImage{subFix}\",rawColorImage)\n",
    "            np.save(f\"depthImage{subFix}\",rawDepthImage)\n",
    "            o3d.io.write_point_cloud(f\"pcd{subFix}.pcd\",downsampledPCD)\n",
    "        return downsampledPCD,rawRGBDImage\n",
    "\n",
    "    def displayImages(self,rgbdImage):\n",
    "        # Displays a depth and color image given the rgbdImage\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.title(\"RealSense Color Image\")\n",
    "        plt.imshow(rgbdImage.color)\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.title(\"RealSense Depth Image\")\n",
    "        plt.imshow(rgbdImage.depth)\n",
    "        plt.show()\n",
    "    \n",
    "    def displayPCD(self,pcd):\n",
    "        # Displays a point cloud given the pcd. Displays camera frame if self.extrinsics != None\n",
    "        flip_transform = [[1, 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]]\n",
    "        pcd.transform(flip_transform)\n",
    "        worldFrame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.25,origin=[0,0,0])\n",
    "        if (self.extrinsics is None) == False:\n",
    "            cameraFrame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.25)\n",
    "            cameraFrame.transform(self.extrinsics)\n",
    "            o3d.visualization.draw([worldFrame,cameraFrame,pcd])\n",
    "        else:\n",
    "            o3d.visualization.draw([worldFrame,pcd])\n",
    "    \n",
    "    def displayStream(self):\n",
    "        # streams and displays the point cloud data in open3d\n",
    "        # pipe,config are stream properties set in the earlier cells \n",
    "        # Streaming loop\n",
    "        pipe,config = self.pipe,self.config\n",
    "        vis = o3d.visualization.Visualizer()\n",
    "        vis.create_window()\n",
    "        framesTaken = 0\n",
    "        displayedPCD = o3d.geometry.PointCloud()\n",
    "        try:\n",
    "            while True:\n",
    "                temp = self.getPCD()[0]\n",
    "                displayedPCD.points = temp.points\n",
    "                displayedPCD.colors = temp.colors\n",
    "                if framesTaken == 0:\n",
    "                    vis.add_geometry(displayedPCD)\n",
    "                    worldFrame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.25,origin=[0,0,0])\n",
    "                    vis.add_geometry(worldFrame)\n",
    "                vis.update_geometry(displayedPCD)\n",
    "                framesTaken += 1\n",
    "                t0 = time.time()\n",
    "                vis.poll_events()\n",
    "                vis.update_renderer()\n",
    "                #time.sleep(5)\n",
    "        except Exception as e:\n",
    "            print(f\"Stream Issue. Exception Raised\")\n",
    "            raise(e)\n",
    "        # closes window when cell is stopped (exception raised)\n",
    "        finally:\n",
    "            vis.destroy_window()\n",
    "            pipe.stop()\n",
    "    \n",
    "\n",
    " \n",
    "# robot_model = RTB_Model()\n",
    "# real = RealSense()\n",
    "# real.initStream()\n",
    "# real.extrinsics = np.array(robot_model.getCameraFrame())\n",
    "# print(real.extrinsics)\n",
    "# pcd,rgbdImage = real.getPCD(True)\n",
    "# real.displayImages(rgbdImage)\n",
    "# real.displayPCD(pcd)\n",
    "# real.pipe.stop()\n",
    "\n",
    "# o3d.visualization.draw()\n",
    "# r.displayStream()\n",
    "# finally:\n",
    "#     r.pipe.stop()\n",
    "# profile = pipe.start(config)\n",
    "# depth_sensor = profile.get_device().first_depth_sensor()\n",
    "# print(\n",
    "# r.displayStream(pipe,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.99468624 -0.09739136 -0.03337981 -0.01548516]\n",
      " [ 0.02688587  0.06724571 -0.99737413 -0.28035581]\n",
      " [ 0.09938027 -0.99297177 -0.06426993  0.17600367]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "[Open3D DEBUG] Format auto File pcd1.pcd\n",
      "[Open3D DEBUG] PCD header indicates 4 fields, 16 bytes per point, and 328964 points in total.\n",
      "[Open3D DEBUG] x, F, 4, 1, 0\n",
      "[Open3D DEBUG] y, F, 4, 1, 4\n",
      "[Open3D DEBUG] z, F, 4, 1, 8\n",
      "[Open3D DEBUG] rgb, F, 4, 1, 12\n",
      "[Open3D DEBUG] Compression method is 1.\n",
      "[Open3D DEBUG] Points: yes;  normals: no;  colors: yes\n",
      "[Open3D DEBUG] Read geometry::PointCloud: 328964 vertices.\n",
      "[Open3D DEBUG] LoadMaterialFromFile(): /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/open3d//resources/defaultLit.filamat\n",
      "[Open3D DEBUG] LoadMaterialFromFile(): /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/open3d//resources/defaultLitTransparency.filamat\n",
      "[Open3D DEBUG] LoadMaterialFromFile(): /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/open3d//resources/defaultLitSSR.filamat\n",
      "[Open3D DEBUG] LoadMaterialFromFile(): /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/open3d//resources/defaultUnlit.filamat\n",
      "[Open3D DEBUG] LoadMaterialFromFile(): /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/open3d//resources/defaultUnlitTransparency.filamat\n",
      "[Open3D DEBUG] LoadMaterialFromFile(): /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/open3d//resources/depth.filamat\n",
      "[Open3D DEBUG] LoadMaterialFromFile(): /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/open3d//resources/unlitGradient.filamat\n",
      "[Open3D DEBUG] LoadMaterialFromFile(): /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/open3d//resources/normals.filamat\n",
      "[Open3D DEBUG] LoadMaterialFromFile(): /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/open3d//resources/unlitSolidColor.filamat\n",
      "[Open3D DEBUG] LoadMaterialFromFile(): /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/open3d//resources/unlitBackground.filamat\n",
      "[Open3D DEBUG] LoadMaterialFromFile(): /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/open3d//resources/infiniteGroundPlane.filamat\n",
      "[Open3D DEBUG] LoadMaterialFromFile(): /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/open3d//resources/unlitLine.filamat\n",
      "[Open3D DEBUG] LoadMaterialFromFile(): /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/open3d//resources/unlitPolygonOffset.filamat\n",
      "[Open3D DEBUG] Trying to destroy default resource [Texture, 1, hash: 655361]. Nothing will happen.\n",
      "[Open3D DEBUG] Trying to destroy default resource [Texture, 1, hash: 655361]. Nothing will happen.\n",
      "[Open3D DEBUG] Trying to destroy default resource [Texture, 1, hash: 655361]. Nothing will happen.\n",
      "[Open3D DEBUG] Trying to destroy default resource [Texture, 1, hash: 655361]. Nothing will happen.\n",
      "[Open3D DEBUG] Trying to destroy default resource [Texture, 1, hash: 655361]. Nothing will happen.\n",
      "[Open3D DEBUG] Trying to destroy default resource [Texture, 1, hash: 655361]. Nothing will happen.\n"
     ]
    }
   ],
   "source": [
    "robot_model = RTB_Model()\n",
    "real = RealSense()\n",
    "real.extrinsics = np.array(robot_model.getCameraFrame())\n",
    "print(real.extrinsics)\n",
    "pcd = o3d.io.read_point_cloud(\"pcd1.pcd\")\n",
    "real.displayPCD(pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 80\u001b[0m\n\u001b[1;32m     76\u001b[0m         bluePCD\u001b[38;5;241m.\u001b[39mpaint_uniform_color([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     77\u001b[0m         o3d\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mdraw([redPCD,yellowPCD,bluePCD])\n\u001b[0;32m---> 80\u001b[0m detector \u001b[38;5;241m=\u001b[39m \u001b[43mObjectDetection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m rgbdImage \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mreal\u001b[38;5;241m.\u001b[39mtakeImages()\n\u001b[1;32m     82\u001b[0m depthImage,colorImage \u001b[38;5;241m=\u001b[39m rgbdImage\u001b[38;5;241m.\u001b[39mdepth,rgbdImage\u001b[38;5;241m.\u001b[39mcolor\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36mObjectDetection.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m robot_model \u001b[38;5;241m=\u001b[39m RTB_Model()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreal \u001b[38;5;241m=\u001b[39m RealSense()\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreal\u001b[38;5;241m.\u001b[39mextrinsics \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(robot_model\u001b[38;5;241m.\u001b[39mgetCameraFrame())\n",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m, in \u001b[0;36mRealSense.initConnection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitConnection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Initializes connection to realsense, sets pipe,config values\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipe \u001b[38;5;241m=\u001b[39m \u001b[43mrs\u001b[49m\u001b[38;5;241m.\u001b[39mpipeline()\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m rs\u001b[38;5;241m.\u001b[39mconfig()\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Getting information about the connected realsense model (device object) - D405\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rs' is not defined"
     ]
    }
   ],
   "source": [
    "class ObjectDetection():\n",
    "    def __init__(self):\n",
    "        robot_model = RTB_Model()\n",
    "        self.real = RealSense()\n",
    "        self.real.initConnection()\n",
    "        self.real.extrinsics = np.array(robot_model.getCameraFrame())\n",
    "    \n",
    "    def getMask(self,colorImage,minHSV,maxHSV):\n",
    "        # :img 3-channel rgb image as numpy array\n",
    "        # :minHSV minimum hsv bound\n",
    "        # :maxHSV max hsv bound\n",
    "        # Segments image based on bounds in hsv color space\n",
    "        # returns a 1-channel binary numpy array that results from segmentation\n",
    "        # convert image to hsv\n",
    "        hsv = cv.cvtColor(colorImage,cv.COLOR_RGB2HSV)\n",
    "        mask = cv.inRange(colorImage,hsvBounds[0],hsvBounds[1])\n",
    "        # binary closing with 10 x 10 kernel (dilation then erosion) to fill in gaps\n",
    "        closedMask = nd.binary_closing(mask,np.ones((10,10)),iterations=5).astype(int)\n",
    "        return closedMask\n",
    "    \n",
    "    \n",
    "    def colorSegmentation(self,colorImage,depthImage):\n",
    "        # Takes a depth and color image from the realsense and processes the images\n",
    "        colorImage,depthImage = RGBD_Image.color,RGBD_Image.depth\n",
    "        redMask = getMask(colorImage,(0,110,20),(15,255,255))\n",
    "        yellowMask = getMask(colorImage,(20,90,20),(35,255,255))\n",
    "        blueMask = getMask(colorImage,(95,90,20),(130,255,255)) \n",
    "        \n",
    "        redDepthImage = np.multiply(depthImage,closedRedMask)\n",
    "        yellowDepthImage = np.multiply(depthImage,closedYellowMask)\n",
    "        blueDepthImage = np.multiply(depthImage,closedBlueMask)\n",
    "        \n",
    "        # SEGMENT PCD INTO RED,YELLOW,BLUE BLOCKS    \n",
    "        depth_scale = self.real.depth_scale\n",
    "        \n",
    "        # Create Segmented RGBD Images for Each Color\n",
    "        redRGDB_Image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "            o3d.geometry.Image(colorImage),\n",
    "            o3d.geometry.Image(np.array(redDepthImage).astype('uint16')),\n",
    "            convert_rgb_to_intensity=False,\n",
    "            depth_scale=1/depth_scale\n",
    "        )\n",
    "        \n",
    "        yellowRGDB_Image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "            o3d.geometry.Image(colorImage),\n",
    "            o3d.geometry.Image(np.array(yellowDepthImage).astype('uint16')),\n",
    "            convert_rgb_to_intensity=False,\n",
    "            depth_scale=1/depth_scale\n",
    "        )\n",
    "    \n",
    "        blueRGBD_Image = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "            o3d.geometry.Image(colorImage),\n",
    "            o3d.geometry.Image(np.array(blueDepthImage).astype('uint16')),\n",
    "            convert_rgb_to_intensity=False,\n",
    "            depth_scale=1/depth_scale)\n",
    "\n",
    "        # Create Point Clouds for Each Color\n",
    "        redPCD = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "            redRGDB_Image,\n",
    "            self.real.instrinsics,\n",
    "            project_valid_depth_only=True\n",
    "        )\n",
    "        yellowPCD = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "            yellowRGDB_Image,\n",
    "            self.real.instrinsics,\n",
    "            project_valid_depth_only=True\n",
    "        )\n",
    "        bluePCD = o3d.geometry.PointCloud.create_from_rgbd_image(\n",
    "            blueRGBD_Image,\n",
    "            self.real.instrinsics,\n",
    "            o3d.camera.PinholeCameraIntrinsic(320,240,320,240,160,120),\n",
    "            project_valid_depth_only=True\n",
    "        )\n",
    "        redPCD.paint_uniform_color([1,0,0])\n",
    "        yellowPCD.paint_uniform_color([0,1,0])\n",
    "        bluePCD.paint_uniform_color([0,0,1])\n",
    "        o3d.visualization.draw([redPCD,yellowPCD,bluePCD])\n",
    "\n",
    "        \n",
    "detector = ObjectDetection()\n",
    "rgbdImage = detector.real.takeImages()\n",
    "depthImage,colorImage = rgbdImage.depth,rgbdImage.color\n",
    "detector.colorSegmentation(colorImage,depthImage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_model = RTB_Model()\n",
    "real = RealSense()\n",
    "print(real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(real.extrinsics is None) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
